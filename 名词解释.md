* LLM
大语言模型（LLM, Large Language Model）是一种具有大量参数的 AI 语言模型，能够执行多种复杂且实用的任务。这些模型通过大量文本数据进行训练，能够生成类人文本、回答问题、总结信息等。Claude 是基于大语言模型的对话助手，并通过人类反馈强化学习（RLHF, Reinforcement Learning from Human Feedback）进一步训练，以更有帮助、诚实且无害。

* Pretraining
预训练（Pretraining）是语言模型在大规模无标注文本数据集上进行的初始训练过程。以 Claude 为例，自回归语言模型会根据文档中的上下文来预测下一个词。预训练模型本身并不擅长回答问题或遵循指令，通常需要复杂的提示工程来引导出预期的行为。通过微调（Fine-tuning）和人类反馈强化学习（RLHF），这些预训练模型可以进一步优化，从而在广泛任务中变得更加有用。

* Fine-tuning
微调（Fine-tuning）是通过额外数据进一步训练预训练模型（Pretraining）的过程。这会使模型开始模仿微调数据集中的模式和特征。微调可以帮助模型适应特定领域、任务或写作风格，但需要仔细考虑微调数据的质量及其对模型性能和潜在偏见的影响。

* SFT (Supervised Fine-Tuning)
SFT 是一种用于语言模型优化的技术，它通过使用标注好的数据集对模型进行进一步训练。相比预训练过程中的无监督学习，SFT 专注于让模型在特定任务上表现更好。在监督微调中，模型根据人为标注的输入和输出对进行学习，从而提高其在回答问题、完成任务或遵循指令等特定场景中的表现。SFT 经常用于帮助模型理解更复杂的任务要求，使其生成的输出更加符合预期。

* LORA（Low-Rank Adaptation）
LORA 是一种微调大型预训练语言模型的技术。它通过在模型的某些权重矩阵上引入低秩分解来降低模型更新所需的参数数量。传统的微调需要更新模型的所有参数，而 LORA 只微调一部分参数，这大大减少了微调的计算成本和存储需求，同时保持了模型的性能。LORA 在特定任务或数据集上的微调表现良好，因为它可以灵活地适应新的任务要求，而不需要重新训练整个模型。

* QLORA（Quantized Low-Rank Adaptation）
QLORA（量化低秩适应）是一种用于大语言模型的微调技术，它通过对模型权重进行低秩分解和量化来减少微调的计算开销，同时保持性能。这种方法能够在保持模型准确性的同时，显著降低内存需求和计算复杂度，因此特别适用于在有限的资源下微调超大规模模型。

* QLORA 的主要特点是：

低秩适应（Low-Rank Adaptation, LORA）：通过对模型权重的低秩分解，QLORA 可以仅对少量参数进行微调，这样可以在节省计算资源的同时仍能有效捕捉任务相关的模式。
量化（Quantization）：QLORA 使用 4-bit 或更低精度的量化技术来减少模型的存储和计算要求。量化技术通过缩减模型中存储和处理的参数位数，能够降低硬件负载，而不显著影响模型的性能。
高效微调：QLORA 可以在不完全重训练模型的情况下进行微调，尤其适用于资源受限的场景，例如边缘设备或中小型研究团队。
QLORA 技术的出现使得对大型预训练模型进行特定任务的微调变得更加可行。

* RLHF（Reinforcement Learning from Human Feedback）
来自人类反馈的强化学习（RLHF, Reinforcement Learning from Human Feedback）是一种用于训练预训练语言模型的技术，使其行为更加符合人类的偏好。这种训练方式可以帮助模型更有效地执行指令，或表现得更像聊天机器人。人类反馈的过程包括对多个文本样本进行排序，强化学习会鼓励模型倾向于生成与高排名样本相似的输出。Claude 已通过 RLHF 进行训练，使其成为一个更加有用的助手。更多详情可以参考 Anthropic 的论文: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback。

* PPO (Proximal Policy Optimization)
PPO（近端策略优化，Proximal Policy Optimization）是一种强化学习算法，常用于优化语言模型的行为，尤其是在结合人类反馈的训练中（如 RLHF）。PPO 的核心目标是通过小步调整策略（policy），在提高模型性能的同时保持训练的稳定性。它通过限制每次策略更新的幅度，避免模型偏离过远，从而在效率和稳定性之间取得平衡。

核心机制：PPO 使用代理目标函数（surrogate objective function），允许多次小批量更新，而非传统政策梯度方法的一次性更新。它通过剪切机制（clipping）限制策略更新幅度，确保新策略不会偏离旧策略过远，从而维持训练稳定性。
优势函数：PPO 利用优势函数（advantage function），计算特定动作相对于平均动作的好处，降低梯度估计的方差。这通过一个单独的价值模型（critic model）实现，价值模型评估状态或动作的价值。
应用场景：PPO 广泛用于机器人运动模拟、游戏 AI 训练等任务。在 LLM 训练中，如 Claude 的 RLHF 阶段，PPO 被用来优化模型以生成更符合人类偏好的输出。例如，Proximal Policy Optimization (PPO) - Hugging Face 详细描述了其在 RLHF 中的应用。
优点：PPO 样本效率高，易于实现，适合标准深度学习框架，适用于离散或连续动作空间。

* GRPO (Generalized Proximal Policy Optimization)
GRPO（广义近端策略优化，Generalized Proximal Policy Optimization）是一种较新的 RL 算法，特别设计用于处理 LLM 在不同群体偏好下的稳健性问题。它属于直接偏好优化方法，无需传统 RLHF 中的单独奖励模型。

核心机制：GRPO 直接基于偏好数据（如成对比较）优化策略，省去训练奖励模型的步骤。它采用群体稳健性方法，目标是最大化最差群体的性能，确保模型对所有用户群体公平。通过自适应加权，优先考虑表现较差的群体，动态调整优化过程。
无奖励模型：与 PPO 不同，GRPO 不需要价值模型，减少了约 50% 的内存和计算开销。这种 reward-free 方式简化了训练流程，特别适合资源受限场景。
群体稳健性：GRPO 关注偏好数据的多样性，例如不同人口统计学群体或公司团队的偏好，确保模型输出对所有群体都有效。例如，Group Robust Preference Optimization in Reward-free RLHF - arXiv 提出了 GRPO 的理论框架。
应用实例：GRPO 被 DeepSeek R1 模型采用，特别是在其 R1-zero 版本中，跳过 SFT 阶段，直接从基础模型（DeepSeek V3）开始 RL，显著提升计算效率。
为了更清晰地对比两者，以下表格总结了关键差异：

特性	PPO	GRPO
价值模型	需要单独的价值模型（critic model）	无需价值模型，减少计算开销
优化方式	基于奖励模型，最大化预期奖励	直接基于偏好数据，无需奖励模型
稳健性	关注整体性能，稳定性通过剪切机制保证	强调群体稳健性，最大化最差群体性能
计算效率	较高，但价值模型增加内存需求	更高，约 50% 计算和内存节省
应用场景	传统 RLHF，适合标准任务	新兴场景，特别适合多样化偏好数据处理
* RAG (Retrieval augmented generation)
检索增强生成（RAG, Retrieval augmented generation）是一种结合信息检索与语言模型生成的技术，用以提升生成文本的准确性和相关性。在 RAG 中，语言模型会结合外部知识库或文档集，并将这些信息传递到上下文窗口中。数据会在运行时根据模型接收的查询进行检索，尽管模型本身不一定负责数据检索（但可以通过工具使用（tool use）和检索功能来实现）。当生成文本时，首先从知识库中检索相关信息，并与原始查询一起传递给模型。模型使用这些信息来指导输出内容，从而访问并利用训练数据之外的知识，减少对记忆的依赖，提升生成内容的准确性。RAG 在需要最新信息、特定领域知识或需要明确引用来源的任务中非常有用，但 RAG 的有效性取决于外部知识库的质量及其在运行时检索到的相关性。

* Tokens
Token（Token）是语言模型中最小的独立单位，可以对应于单词、子词、字符，甚至是字节（例如 Unicode）。对于 Claude 来说，一个 Token 大约对应 3.5 个英文字符，具体数字可能会根据所使用的语言有所不同。在与语言模型交互时，Token 通常在“文本”层级是隐藏的，但在检查输入和输出时则非常重要。当 Claude 被提供文本时，这些文本（由一系列字符组成）会被编码为一系列 Token，供模型处理。较大的 Token 可以提升推理和预训练过程中的数据效率，而较小的 Token 则使模型能够处理不常见或前所未见的词汇。Token 化方法的选择会影响模型的性能、词汇量大小以及模型处理未见词汇的能力。

* Context window
“上下文窗口（Context window）”指的是语言模型在生成新文本时可以回溯并参考的文本量。这与模型接受训练时使用的大规模数据集不同，而是模型的“工作记忆”。较大的上下文窗口有助于模型理解并回应更加复杂且冗长的提示，而较小的上下文窗口则可能限制模型处理较长提示或在长时间对话中保持连贯性的能力。

* Temperature
温度（Temperature）是一个用于控制模型生成预测时随机性的参数。较高的温度会带来更多的创造性和多样性输出，允许生成多种短语变体，尤其在虚构创作中表现显著。较低的温度会产生更加保守和确定的输出，趋向于选择最可能的短语和答案。通过调整温度，用户可以引导模型探索罕见或不寻常的词汇选择和序列，而不仅仅选择最可能的预测。

* Latency
在生成式 AI 和大语言模型中，延迟（Latency）指的是模型在接收提示后生成响应所需的时间。这是从提交提示到收到生成文本的延迟时间。较低的延迟意味着更快的响应时间，这对于实时应用、聊天机器人和交互体验至关重要。影响延迟的因素包括模型大小、硬件性能、网络状况以及提示和生成文本的复杂性。

* TTFT (Time to First Token)
首个 Token 的时间（TTFT, Time to First Token）是衡量语言模型在接收提示后生成首个 Token 所需时间的性能指标。它在互动应用、聊天机器人和实时系统中是评估模型响应速度的重要指标，用户通常希望快速得到初始反馈。较低的 TTFT 表示模型能够更快开始生成响应，从而提供更流畅和有吸引力的用户体验。影响 TTFT 的因素包括模型大小、硬件性能、网络条件以及提示的复杂性。
